{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhayu/Downloads/SmartLLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.llm_dataset import LLMDataset, LLMType\n",
    "\n",
    "d = LLMDataset.from_file(dir=\"generated\", file=\"all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LLMType.LLM1: 'llm1'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLMType.from_substr(\"LLM1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhayu/Downloads/SmartLLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.messages import MessagesList, AlpacaMessagesList, Messages, AlpacaMessages, messages_list_factory, ConversationFormat\n",
    "from models.llm_dataset import LLMDataset, ConversationFormat, LLMType, LLMDatasetWithTypes, DatasetRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LLMDatasetWithTypes(rows=38654),\n",
       " LLMDatasetWithTypes(rows=29281),\n",
       " LLMDatasetWithTypes(rows=0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmd_hindi = LLMDataset.from_file(full_path=\"hindi.json\")\n",
    "llmd_spanish = LLMDataset.from_file(full_path=\"spanish.json\")\n",
    "llmd_italian = LLMDataset.from_file(full_path=\"italian.json\")\n",
    "llmd_telugu = LLMDataset.from_file(full_path=\"telugu.json\")\n",
    "\n",
    "llmd_hindi = LLMDataset(rows=[DatasetRow(**(x.model_dump() | {\"language\": \"hindi\"})) for x in llmd_hindi])\n",
    "llmd_spanish = LLMDataset(rows=[DatasetRow(**(x.model_dump() | {\"language\": \"spanish\"})) for x in llmd_spanish])\n",
    "llmd_italian = LLMDataset(rows=[DatasetRow(**(x.model_dump() | {\"language\": \"italian\"})) for x in llmd_italian])\n",
    "llmd_telugu = LLMDataset(rows=[DatasetRow(**(x.model_dump() | {\"language\": \"telugu\"})) for x in llmd_telugu])\n",
    "\n",
    "llmd: LLMDataset = llmd_hindi + llmd_spanish + llmd_italian + llmd_telugu\n",
    "del llmd_hindi, llmd_spanish, llmd_italian, llmd_telugu\n",
    "llmd1 = llmd.get_llm_type_rows(LLMType.LLM1)\n",
    "llmd2 = llmd.get_llm_type_rows(LLMType.LLM2)\n",
    "llmd3 = llmd.get_llm_type_rows(LLMType.LLM3)\n",
    "del llmd\n",
    "llmd1, llmd2, llmd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = LLMDataset.from_jsonl(\"dataset/llm1_alpaca.jsonl\").get_llm_type_rows(LLMType.LLM1)\n",
    "d2 = LLMDataset.from_jsonl(\"dataset/llm2_alpaca.jsonl\").get_llm_type_rows(LLMType.LLM2)\n",
    "d3 = LLMDataset.from_jsonl(\"dataset/llm3_alpaca.jsonl\").get_llm_type_rows(LLMType.LLM3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = llmd1 + d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = d1.to_messages().to_dataset()\n",
    "del d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/subhayu/Downloads/SmartLLM'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "subprocess.Popen(f'{os.getcwd()}{sys.executable} -m ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "hindi       12199\n",
       "spanish      9786\n",
       "italian      4913\n",
       "telugu       2383\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmd2.to_messages().to_dataset().to_pandas().value_counts([\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dt1 = Dataset.from_json(\"/home/subhayu/Downloads/SmartLLM/llm1_50k_multi.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import datetime_from_uid, get_ts_filename, datetime_from_tsfile, parts_from_tsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d1 + llmd1\n",
    "d2 = d2 + llmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dd = DatasetDict(\n",
    "    llm1_alpaca=d1.to_messages().to_dataset(),\n",
    "    llm2_alpaca=d2.to_messages().to_dataset(),\n",
    "    llm3_alpaca=d3.to_messages().to_dataset(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMDatasetWithTypes(rows=100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 84/84 [00:00<00:00, 280.76ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 51/51 [00:00<00:00, 56.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.74s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 135.65ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "README.md: 100%|██████████| 655/655 [00:00<00:00, 2.73MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/subhayu-qxlabai/SmartLLM/commit/f08af88864e8338c99314b63ded125ab9443191b', commit_message='Upload dataset', commit_description='', oid='f08af88864e8338c99314b63ded125ab9443191b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.push_to_hub(\"subhayu-qxlabai/SmartLLM\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = LLMDataset.from_jsonl(\"/home/subhayu/Downloads/SmartLLM/llm1_alpaca(1) 1.jsonl\").get_llm_type_rows(LLMType.LLM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def extract_keywords(sentence: str):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    keywords = {word for word, tag in tagged_words if tag[0] in [\"N\", \"J\", \"V\", \"I\"]}\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "def extract_keywords(sentences: str, with_scores: bool = False):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_sentences(sentences)\n",
    "    if not with_scores:\n",
    "        return r.get_ranked_phrases()\n",
    "    return r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where',\n",
       " 'can',\n",
       " 'I',\n",
       " '[FIND]',\n",
       " 'high-resolution',\n",
       " '[IMAGES]',\n",
       " 'of',\n",
       " '[CANCER]',\n",
       " '[CELLS]',\n",
       " 'for',\n",
       " 'a',\n",
       " '[COLLEGE]',\n",
       " '[REPORT]',\n",
       " '?']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence: str = d1.rows[0].output.question\n",
    "sentences = word_tokenize(sentence)\n",
    "kws = extract_keywords(sentences)\n",
    "sorted(kws, key=lambda x: sentence.index(x[1]))\n",
    "[f\"[{x.upper()}]\" if x in kws else x for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cancer', 'cells', 'college', 'images', 'report'},\n",
       " {'Look',\n",
       "  'SEARCH',\n",
       "  'Use',\n",
       "  'archives',\n",
       "  'cancer',\n",
       "  'cells.|CHECK',\n",
       "  'databases',\n",
       "  'image',\n",
       "  'images',\n",
       "  'organization',\n",
       "  'purposes.|VISIT',\n",
       "  'resources',\n",
       "  'rights',\n",
       "  'websites'})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(extract_keywords(d1.rows[0].output.question)) - set(extract_keywords('|'.join(d1.rows[0].output.tasks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llm_dataset import LLMType, LLMDataset\n",
    "\n",
    "d2 = LLMDataset.from_jsonl(\"dataset/llm2_alpaca.jsonl\")[32000:].get_llm_type_rows(LLMType.LLM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "extracts = list(chain(*[r.output.get_extracts_with_functions() for r in d2.rows if r.output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractGenerator num tokens: 2186\n"
     ]
    }
   ],
   "source": [
    "from dataset_gen.smart_llm.extract_generator import ExtractGenerator\n",
    "\n",
    "resp = ExtractGenerator().generate(extracts[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [{'id': 'function_1',\n",
       "   'name': 'PEACE_TREATY_ANALYSIS',\n",
       "   'parameters': [{'name': 'treaty_name', 'value': '2023 peace initiatives'},\n",
       "    {'name': 'context',\n",
       "     'value': 'Identify and analyze the latest peace treaties and agreements signed or proposed in the year 2023.'}]},\n",
       "  {'id': 'function_2',\n",
       "   'name': 'PROMOTE_PEACE_INITIATIVES',\n",
       "   'parameters': [{'name': 'region', 'value': 'global'}]}],\n",
       " 'schema': [{'name': 'summary_of_initiatives',\n",
       "   'type': 'string',\n",
       "   'description': 'A combined description of the latest peace initiatives and agreements.'}]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "xd = dict(sorted([x for x in extracts if len(x.functions) == 2][4].model_dump().items(), key=lambda item: item[0]))\n",
    "json.dumps(xd)\n",
    "xd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
