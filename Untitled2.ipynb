{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0739e526-17b3-4330-ab0c-bd926b710151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n",
      "------------Loading LLM1-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4775e9caf2dc4147be9f20eea89edec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helpers.text_utils import TextUtils\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "print(\"------------Loading LLM1-----------\")\n",
    "\n",
    "\n",
    "# finetuned_model = AutoModelForCausalLM.from_pretrained(\"vipinkatara/mLLM1_model\", device_map='auto', use_cache=False,offload_folder=folder_offload, offload_state_dict=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vipinkatara/mLLM1_model\", device_map='auto',offload_folder=folder_offload, offload_state_dict=True)\n",
    "\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"vipinkatara/mLLM1_model\", device_map='auto', use_cache=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vipinkatara/mLLM1_model\", device_map='auto')\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    " \n",
    "# text = \"Bonjour tout le monde\"  # Example text\n",
    "# language = detect(text)\n",
    "# print(\"Detected language:\", language)\n",
    "\n",
    "system_messages = [\n",
    "    \"Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false.\"\n",
    "]\n",
    "\n",
    "def get_prompt(_input):\n",
    "    if isinstance(_input, str):\n",
    "        _input = {\"question\": _input}\n",
    "        # prompt_template = f\"\"\"[INST] {_input} [/INST] ### Response: {{question:{_input},tasks:[string,string],can_i_answer:string}}\"\"\"\n",
    "        prompt_template = f\"\"\"<<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {_input} [/INST] \"\"\"\n",
    "        return prompt_template\n",
    "\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "    generated_ids = finetuned_model.generate(\n",
    "        **model_inputs, \n",
    "        max_new_tokens=8096, \n",
    "        do_sample=False, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "    start_string=prompt + ' {\"output\": \"'\n",
    "    end_string='\"}'+tokenizer.eos_token\n",
    "    print(f\"{decoded_output[0]=}\")\n",
    "    response = TextUtils.get_middle_text(decoded_output[0], start_string, end_string).strip()\n",
    "    print(f\"{response=}\")\n",
    "    response = response.replace('\\\\\"', '\"')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb05b7e4-abcc-48ac-a202-7aff4d1d026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question='Who is the president of India?'\n",
    "prompt=get_prompt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f2e647-a585-4307-a9e4-37d4cc3d4256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you\\'re charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {\"question\": \"Who is the president of India?\"} [/INST] '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "123c0e7f-e092-45e2-9132-6b3b7d5345cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_output[0]='<<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you\\'re charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {\"question\": \"Who is the president of India?\"} [/INST]  {\"output\": \"{\\\\\"question\\\\\":\\\\\"Who is the president of India?\\\\\",\\\\\"tasks\\\\\":[],\\\\\"can_i_answer\\\\\":true}\"}</s>'\n",
      "response='{\\\\\"question\\\\\":\\\\\"Who is the president of India?\\\\\",\\\\\"tasks\\\\\":[],\\\\\"can_i_answer\\\\\":true}'\n"
     ]
    }
   ],
   "source": [
    "actual_output=get_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0520589-1796-4d4d-9d52-52633c498c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"question\":\"Who is the president of India?\",\"tasks\":[],\"can_i_answer\":true}'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
