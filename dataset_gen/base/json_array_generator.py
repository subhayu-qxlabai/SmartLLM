import json
from pathlib import Path
from itertools import chain
from typing import Callable

from helpers.call_openai import call_openai_api
from dataset_gen.base.model_validator import BaseModelValidator
from helpers.utils import (
    num_tokens_from_messages,
    clean_json_str,
    try_json_loads,
    get_ts_filename,
    join_lists_from_dir,
)

DEFAULT_DUMP_DIR = Path("generated/json_array")

class JSONArrayGenerator(BaseModelValidator):
    def __init__(
        self,
        dump_dir: str | Path = DEFAULT_DUMP_DIR,
        file_prefix: str = "x",
        system_prompt: str = None,
        example_messages: list[dict[str, str]] = [],
        openai_func: Callable = call_openai_api,
        max_tokens: int = 4000,
        verbose: bool = True,
    ):
        """
        Initializes the class with the specified parameters.

        Args:
            dump_dir (str | Path): The directory to dump files in.
            file_prefix (str): The prefix for the file name.
            system_prompt (str): The prompt for the system.
            example_messages (list[dict[str, str]]): A list of example messages.
            openai_func (Callable): The function to call the OpenAI API.
            verbose (bool): Whether to display verbose output.

        Returns:
            None
        """
        self.dump_dir = Path(dump_dir)
        self.file_prefix = (
            file_prefix if file_prefix.endswith(".json") else f"{file_prefix}.json"
        )
        self.system_prompt = system_prompt or "You are a smart assistant."
        self.example_messages = example_messages or []
        self.generated: list[str | dict] = []
        self.call_openai_func = openai_func
        self.max_tokens = max_tokens
        self.verbose = verbose
        super().__init__(verbose=verbose)

    def get_dump_file(self):
        return self.dump_dir / get_ts_filename(self.file_prefix)
    
    def load(self) -> list[str | dict]:
        """
        Loads the list of generated items from a JSON files specified by the `dump_dir` parameter.
        """
        items = join_lists_from_dir(self.dump_dir)
        if all(isinstance(x, str) for x in items):
            items = list(set(items))
        return items

    def dump(self, generated: list[str | dict] = []):
        """
        Writes the list of generated items to a JSON file specified by the dump_file parameter.

        Args:
            generated (list[str | dict], optional): The list of generated items to be written to the file. Defaults to an empty list.

        Returns:
            Path: The path to the JSON dump file.
        """
        dump_file = self.get_dump_file()
        dump_file.parent.mkdir(parents=True, exist_ok=True)
        generated = list(generated) or self.generated
        with open(dump_file, "w") as f:
            json.dump(generated, f, indent=2)
        return dump_file

    def _get_openai_contents(self, last_user_message: str, n: int = 1) -> list[str]:
        """
        Get the contents generated by OpenAI.

        Args:
            last_user_message (str): The last user message to be included in the conversation.
            n (int, optional): The number of completions to generate. Defaults to 1.

        Returns:
            list[str]: A list of generated contents based on the last user message.
        """
        n = 1 if not isinstance(n, int) or n < 1 else n
        messages = [
            {"role": "system", "content": self.system_prompt},
            *self.example_messages,
            {"role": "user", "content": last_user_message},
        ]
        num_tokens = num_tokens_from_messages(messages)
        if num_tokens > self.max_tokens:
            if self.verbose:
                print(
                    f"Num tokens: {num_tokens} exceeds max_tokens: {self.max_tokens}."
                )
            return []
        if self.verbose:
            print(f"{self.__class__.__name__} num tokens: {num_tokens}")

        generated = self.call_openai_func(messages=messages, temperature=0.7, n=n)
        if generated is None:
            return []
        return [x.message.content for x in generated.choices]

    def _generate_responses(
        self, last_user_message: str, n: int = 1
    ):
        """
        Generate text based on the input text, with optional settings for generation.

        Args:
            text (str): The input text for generation.
            n (int, optional): The number of outputs to generate. Defaults to 1.

        Returns:
            list[str]: The generated text.
        """
        generated = self._get_openai_contents(last_user_message, n) or []
        generated: list[str | dict] = list(
            chain(*[try_json_loads(clean_json_str(x)) for x in generated])
        )
        return generated

    def generate_and_dump(self, text: str, n: int = 1, dump: bool = False):
        """
        Generates responses based on the input text and optionally dumps them. 

        Args:
            text (str): The input text for generating responses.
            n (int): The number of responses to generate. Defaults to 1.
            dump (bool): Whether to dump the generated responses. Defaults to False.

        Returns:
            list: The list of generated responses.
        """
        generated = self._generate_responses(text, n)
        if generated:
            self.generated.extend(generated)
            if dump:
                self.dump(generated)
        else:
            return [None]*n
        return generated
