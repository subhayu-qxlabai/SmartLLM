{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4c4633-0a66-45b9-a526-af057848de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.local/lib/python3.11/site-packages/deepeval/__init__.py:41: UserWarning: You are using deepeval version 0.21.23, however version 0.21.24 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "/home/azureuser/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BloomForCausalLM\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from helpers.text_utils import TextUtils\n",
    "from deepeval.metrics import GEval,HallucinationMetric,AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "import re\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from helpers.call_openai import choosed_gpt4_key\n",
    "from deepeval.metrics import BaseMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22671f3-73b6-43b8-865c-55a32cff125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        # print(chat_model.invoke(prompt).content)\n",
    "        res=chat_model.invoke(prompt).content\n",
    "        print(f\"{res=}\")\n",
    "        return res\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        print(f\"{res.content=}\")\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "llm_api = choosed_gpt4_key()[\"api\"]\n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-12-01-preview\",\n",
    "    azure_deployment=llm_api[\"model_id\"],\n",
    "    azure_endpoint=llm_api[\"endpoint\"],\n",
    "    openai_api_key=llm_api[\"key1\"],\n",
    ")\n",
    "llm1=AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b5f1c2-4c54-411e-b555-6a56def82b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CustomMetric(BaseMetric):\\n    def __init__(self, threshold=0.5):\\n        self.threshold = threshold\\n        self.async_mode = False\\n\\n    def measure(self, test_case: LLMTestCase):\\n        # Custom evaluation logic goes here\\n        input_question = test_case.input[\\'question\\']\\n        actual_can_i_answer = test_case.actual_output[\\'can_i_answer\\']\\n        actual_tasks = test_case.actual_output[\\'tasks\\']\\n\\n        # Your custom evaluation logic\\n        question_accuracy = 1 if input_question == test_case.actual_output[\\'question\\'] else 0\\n        can_i_answer_accuracy = 1 if actual_can_i_answer == test_case.input[\\'can_i_answer\\'] else 0\\n        tasks_accuracy = None if actual_can_i_answer else (1 if actual_tasks == test_case.input[\\'tasks\\'] else 0)\\n\\n        total_score = (question_accuracy + can_i_answer_accuracy + tasks_accuracy) / 3\\n\\n        # Set self.success and self.score\\n        self.score = total_score\\n        self.success = total_score >= self.threshold\\n\\n        # Set a reason for the score returned\\n        if self.success:\\n            self.reason = \"Custom metric score is above threshold.\"\\n        else:\\n            self.reason = \"Custom metric score is below threshold.\"\\n\\n        return self.score\\n\\n    def is_successful(self):\\n        return self.success\\n\\n    @property\\n    def __name__(self):\\n        return \"CustomMetric\"\\n\\ncustom_metric = CustomMetric(threshold=0.7)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geval_question_undertsanding_metric = GEval(\n",
    "    model=llm1,\n",
    "    name=\"Question Understanding with Tasks Steps Accuracy\",\n",
    "    # criteria=\"Task Coherence - determine if the tasks key value given in actual output is coherent with the question given in the input.\",\n",
    "    # criteria=\"LanguageTaskUnderstanding - determine if the steps returned in the tasks key value in the output is correct and accurate for the question given in the input. \",\n",
    "    criteria=\"Question Understanding with Tasks Steps Accuracy - determine if it is able to understand the input question given in any language well and has provided accurate steps to answer the question\",\n",
    "    threshold=0.5,\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "geval_format_metric = GEval(\n",
    "    model=llm1,\n",
    "    name=\"Output Format\",\n",
    "    # criteria=\"Task Coherence - determine if the tasks key value given in actual output is coherent with the question given in the input.\",\n",
    "    criteria=\"\"\"Output Format - determine if the output matches the following JSON schema\n",
    "    \n",
    "    {\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"question\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"tasks\": {\n",
    "      \"type\": \"array\"\n",
    "    },\n",
    "    \"can_i_answer\": {\n",
    "      \"type\": \"boolean\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"question\", \"tasks\", \"can_i_answer\"]\n",
    "}\"\"\",\n",
    "    threshold=0.7,\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "\n",
    "# hallucination_metric = HallucinationMetric(threshold=0.5,model=llm1)\n",
    "answerrelevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=llm1,\n",
    "    include_reason=True\n",
    ")\n",
    "'''\n",
    "class CustomMetric(BaseMetric):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.async_mode = False\n",
    "\n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        # Custom evaluation logic goes here\n",
    "        input_question = test_case.input['question']\n",
    "        actual_can_i_answer = test_case.actual_output['can_i_answer']\n",
    "        actual_tasks = test_case.actual_output['tasks']\n",
    "\n",
    "        # Your custom evaluation logic\n",
    "        question_accuracy = 1 if input_question == test_case.actual_output['question'] else 0\n",
    "        can_i_answer_accuracy = 1 if actual_can_i_answer == test_case.input['can_i_answer'] else 0\n",
    "        tasks_accuracy = None if actual_can_i_answer else (1 if actual_tasks == test_case.input['tasks'] else 0)\n",
    "\n",
    "        total_score = (question_accuracy + can_i_answer_accuracy + tasks_accuracy) / 3\n",
    "\n",
    "        # Set self.success and self.score\n",
    "        self.score = total_score\n",
    "        self.success = total_score >= self.threshold\n",
    "\n",
    "        # Set a reason for the score returned\n",
    "        if self.success:\n",
    "            self.reason = \"Custom metric score is above threshold.\"\n",
    "        else:\n",
    "            self.reason = \"Custom metric score is below threshold.\"\n",
    "\n",
    "        return self.score\n",
    "\n",
    "    def is_successful(self):\n",
    "        return self.success\n",
    "\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"CustomMetric\"\n",
    "\n",
    "custom_metric = CustomMetric(threshold=0.7)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2177b29-445f-4bb1-b60c-1209abcef9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n    \"statements\": [\\n        \"{\\\\\"question\\\\\":\\\\\"What are the latest electric vehicle tax \n",
       "credits available in 2023?\\\\\"}\",\\n        \"{\\\\\"tasks\\\\\":[\\\\\"RESEARCH: Look up the latest information on electric \n",
       "vehicle tax credits for the current year\\\\\"]}\",\\n        \"{\\\\\"can_i_answer\\\\\":false}\"\\n    ]\\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n    \"statements\": [\\n        \"{\\\\\"question\\\\\":\\\\\"What are the latest electric vehicle tax \n",
       "credits available in 2023?\\\\\"}\",\\n        \"{\\\\\"tasks\\\\\":[\\\\\"RESEARCH: Look up the latest information on electric \n",
       "vehicle tax credits for the current year\\\\\"]}\",\\n        \"{\\\\\"can_i_answer\\\\\":false}\"\\n    ]\\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n  \"steps\": [\\n    \"Verify if the language of the input question is correctly identified \n",
       "and comprehended.\",\\n    \"Check if the actual output directly addresses the input question\\'s subject matter.\",\\n  \n",
       "\"Assess the logical sequence and relevance of the steps provided in relation to the input question.\",\\n    \"Ensure \n",
       "the steps are structured as a list of strings in the output JSON, adhering to the specified format.\"\\n  ]\\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n  \"steps\": [\\n    \"Verify if the language of the input question is correctly identified \n",
       "and comprehended.\",\\n    \"Check if the actual output directly addresses the input question\\'s subject matter.\",\\n  \n",
       "\"Assess the logical sequence and relevance of the steps provided in relation to the input question.\",\\n    \"Ensure \n",
       "the steps are structured as a list of strings in the output JSON, adhering to the specified format.\"\\n  ]\\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n  \"steps\": [\\n    \"Verify that the Actual Output is a valid JSON object.\",\\n    \"Check \n",
       "whether the Actual Output has \\'question\\', \\'tasks\\', and \\'can_i_answer\\' as the required keys.\",\\n    \"Ensure \n",
       "that the \\'question\\' field in the Actual Output is of type string.\",\\n    \"Confirm that the \\'tasks\\' field in the\n",
       "Actual Output is an array.\",\\n    \"Validate that the \\'can_i_answer\\' field in the Actual Output is a boolean.\"\\n  \n",
       "]\\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n  \"steps\": [\\n    \"Verify that the Actual Output is a valid JSON object.\",\\n    \"Check \n",
       "whether the Actual Output has \\'question\\', \\'tasks\\', and \\'can_i_answer\\' as the required keys.\",\\n    \"Ensure \n",
       "that the \\'question\\' field in the Actual Output is of type string.\",\\n    \"Confirm that the \\'tasks\\' field in the\n",
       "Actual Output is an array.\",\\n    \"Validate that the \\'can_i_answer\\' field in the Actual Output is a boolean.\"\\n  \n",
       "]\\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n    \"verdicts\": [\\n        {\\n            \"verdict\": \"yes\"\\n        },\\n        {\\n       \n",
       "\"verdict\": \"yes\"\\n        },\\n        {\\n            \"verdict\": \"yes\"\\n        }\\n    ]  \\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n    \"verdicts\": [\\n        {\\n            \"verdict\": \"yes\"\\n        },\\n        {\\n       \n",
       "\"verdict\": \"yes\"\\n        },\\n        {\\n            \"verdict\": \"yes\"\\n        }\\n    ]  \\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content=\"The score is 1.00 because the output is directly addressing the input with precision and without \n",
       "including any irrelevant statements. It's spot on! Keep up the good work!\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content=\"The score is 1.00 because the output is directly addressing the input with precision and without \n",
       "including any irrelevant statements. It's spot on! Keep up the good work!\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n  \"score\": 10,\\n  \"reason\": \"The Actual Output meets all the specified criteria. It is a \n",
       "valid JSON with the required keys \\'question\\', \\'tasks\\', and \\'can_i_answer\\'. The \\'question\\' field is a \n",
       "string, the \\'tasks\\' field is an array, and the \\'can_i_answer\\' field is a boolean.\"\\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n  \"score\": 10,\\n  \"reason\": \"The Actual Output meets all the specified criteria. It is a \n",
       "valid JSON with the required keys \\'question\\', \\'tasks\\', and \\'can_i_answer\\'. The \\'question\\' field is a \n",
       "string, the \\'tasks\\' field is an array, and the \\'can_i_answer\\' field is a boolean.\"\\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='{\\n  \"score\": 8,\\n  \"reason\": \"The assistant correctly identified the language of the question and its\n",
       "subject matter, acknowledging that the latest electric vehicle tax credits for 2023 can\\'t be answered using \n",
       "internal capabilities. The output is in the correct JSON structure with the necessary keys: \\'can_i_answer\\' and \n",
       "\\'tasks.\\' However, the task list could include more detailed steps for external research to fully address the \n",
       "question, such as specifying the need to check government or authoritative financial websites, which would make the\n",
       "response more valuable.\"\\n}'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='{\\n  \"score\": 8,\\n  \"reason\": \"The assistant correctly identified the language of the question and its\n",
       "subject matter, acknowledging that the latest electric vehicle tax credits for 2023 can\\'t be answered using \n",
       "internal capabilities. The output is in the correct JSON structure with the necessary keys: \\'can_i_answer\\' and \n",
       "\\'tasks.\\' However, the task list could include more detailed steps for external research to fully address the \n",
       "question, such as specifying the need to check government or authoritative financial websites, which would make the\n",
       "response more valuable.\"\\n}'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='{\\n    \"statements\": [\\n        \"{\\\\\"question\\\\\":\\\\\"Which countries currently offer incentives for \n",
       "purchasing electric vehicles?\\\\\"}\",\\n        \"{\\\\\"tasks\\\\\":[\\\\\"SEARCH: Find a list of countries with incentives for\n",
       "purchasing electric vehicles\\\\\"]}\",\\n        \"{\\\\\"can_i_answer\\\\\":false}\"\\n    ]\\n}'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='{\\n    \"statements\": [\\n        \"{\\\\\"question\\\\\":\\\\\"Which countries currently offer incentives for \n",
       "purchasing electric vehicles?\\\\\"}\",\\n        \"{\\\\\"tasks\\\\\":[\\\\\"SEARCH: Find a list of countries with incentives for\n",
       "purchasing electric vehicles\\\\\"]}\",\\n        \"{\\\\\"can_i_answer\\\\\":false}\"\\n    ]\\n}'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n    \"verdicts\": [\\n        {\\n            \"verdict\": \"yes\"\\n        },\\n        {\\n       \n",
       "\"verdict\": \"yes\"\\n        },\\n        {\\n            \"verdict\": \"yes\"\\n        }\\n    ]\\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n    \"verdicts\": [\\n        {\\n            \"verdict\": \"yes\"\\n        },\\n        {\\n       \n",
       "\"verdict\": \"yes\"\\n        },\\n        {\\n            \"verdict\": \"yes\"\\n        }\\n    ]\\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/azureuser/.local/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='{\\n  \"score\": 8,\\n  \"reason\": \"The actual output successfully identifies the nature of the input \n",
       "question and indicates a logical step to answer it. It correctly formats the response in JSON with the required \n",
       "keys \\'can_i_answer\\' and \\'tasks.\\' However, the output contains additional information not requested by the \n",
       "original question\\'s format, which may be considered extraneous.\"\\n}'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='{\\n  \"score\": 8,\\n  \"reason\": \"The actual output successfully identifies the nature of the input \n",
       "question and indicates a logical step to answer it. It correctly formats the response in JSON with the required \n",
       "keys \\'can_i_answer\\' and \\'tasks.\\' However, the output contains additional information not requested by the \n",
       "original question\\'s format, which may be considered extraneous.\"\\n}'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='The score is 1.00 because the output perfectly matches the given instructions, providing a clear and \n",
       "relevant response with no irrelevant statements.'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='The score is 1.00 because the output perfectly matches the given instructions, providing a clear and \n",
       "relevant response with no irrelevant statements.'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">res.content='```json\\n{\\n  \"score\": 10,\\n  \"reason\": \"The Actual Output is a valid JSON object, contains all \n",
       "required keys (\\'question\\', \\'tasks\\', \\'can_i_answer\\'), the \\'question\\' field is a string, the \\'tasks\\' field \n",
       "is an array, and the \\'can_i_answer\\' field is a boolean.\"\\n}\\n```'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "res.content='```json\\n{\\n  \"score\": 10,\\n  \"reason\": \"The Actual Output is a valid JSON object, contains all \n",
       "required keys (\\'question\\', \\'tasks\\', \\'can_i_answer\\'), the \\'question\\' field is a string, the \\'tasks\\' field \n",
       "is an array, and the \\'can_i_answer\\' field is a boolean.\"\\n}\\n```'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The score is 1.00 because the output is directly addressing the input with precision and without including any irrelevant statements. It's spot on! Keep up the good work!, error: None)\n",
      "  - ✅ Question Understanding with Tasks Steps Accuracy (GEval) (score: 0.8, threshold: 0.5, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The assistant correctly identified the language of the question and its subject matter, acknowledging that the latest electric vehicle tax credits for 2023 can't be answered using internal capabilities. The output is in the correct JSON structure with the necessary keys: 'can_i_answer' and 'tasks.' However, the task list could include more detailed steps for external research to fully address the question, such as specifying the need to check government or authoritative financial websites, which would make the response more valuable., error: None)\n",
      "  - ✅ Output Format (GEval) (score: 1.0, threshold: 0.7, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The Actual Output meets all the specified criteria. It is a valid JSON with the required keys 'question', 'tasks', and 'can_i_answer'. The 'question' field is a string, the 'tasks' field is an array, and the 'can_i_answer' field is a boolean., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: <<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {'question': 'What are the latest electric vehicle tax credits available in 2023?'} [/INST] \n",
      "  - actual output: {\\\"question\\\":\\\"What are the latest electric vehicle tax credits available in 2023?\\\",\\\"tasks\\\":[\\\"RESEARCH: Look up the latest information on electric vehicle tax credits for the current year\\\"],\\\"can_i_answer\\\":false}\n",
      "  - expected output: {\"question\": \"What are the latest electric vehicle tax credits available in 2023?\", \"tasks\": [\"SEARCH: Look up the latest information on electric vehicle tax credits for 2023 from official sources such as government websites or reputable financial publications\"], \"can_i_answer\": false}\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The score is 1.00 because the output perfectly matches the given instructions, providing a clear and relevant response with no irrelevant statements., error: None)\n",
      "  - ✅ Question Understanding with Tasks Steps Accuracy (GEval) (score: 0.8, threshold: 0.5, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The actual output successfully identifies the nature of the input question and indicates a logical step to answer it. It correctly formats the response in JSON with the required keys 'can_i_answer' and 'tasks.' However, the output contains additional information not requested by the original question's format, which may be considered extraneous., error: None)\n",
      "  - ✅ Output Format (GEval) (score: 1.0, threshold: 0.7, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The Actual Output is a valid JSON object, contains all required keys ('question', 'tasks', 'can_i_answer'), the 'question' field is a string, the 'tasks' field is an array, and the 'can_i_answer' field is a boolean., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: <<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {'question': 'Which countries currently offer incentives for purchasing electric vehicles?'} [/INST] \n",
      "  - actual output: {\\\"question\\\":\\\"Which countries currently offer incentives for purchasing electric vehicles?\\\",\\\"tasks\\\":[\\\"SEARCH: Find a list of countries with incentives for purchasing electric vehicles\\\"],\\\"can_i_answer\\\":false}\n",
      "  - expected output: {\"question\": \"Which countries currently offer incentives for purchasing electric vehicles?\", \"tasks\": [\"RESEARCH: Identify a list of countries that offer electric vehicle incentives\", \"COMPARE: Examine the types of incentives each country offers\"], \"can_i_answer\": false}\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TestResult(success=True, metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x7f11b97b62d0>, <deepeval.metrics.g_eval.g_eval.GEval object at 0x7f11adc28390>, <deepeval.metrics.g_eval.g_eval.GEval object at 0x7f11adc2b490>], input=\"<<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {'question': 'What are the latest electric vehicle tax credits available in 2023?'} [/INST] \", actual_output='{\\\\\"question\\\\\":\\\\\"What are the latest electric vehicle tax credits available in 2023?\\\\\",\\\\\"tasks\\\\\":[\\\\\"RESEARCH: Look up the latest information on electric vehicle tax credits for the current year\\\\\"],\\\\\"can_i_answer\\\\\":false}', expected_output='{\"question\": \"What are the latest electric vehicle tax credits available in 2023?\", \"tasks\": [\"SEARCH: Look up the latest information on electric vehicle tax credits for 2023 from official sources such as government websites or reputable financial publications\"], \"can_i_answer\": false}', context=None, retrieval_context=None), TestResult(success=True, metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x7f11af5c0f50>, <deepeval.metrics.g_eval.g_eval.GEval object at 0x7f11b9717610>, <deepeval.metrics.g_eval.g_eval.GEval object at 0x7f11af5c0f90>], input=\"<<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {'question': 'Which countries currently offer incentives for purchasing electric vehicles?'} [/INST] \", actual_output='{\\\\\"question\\\\\":\\\\\"Which countries currently offer incentives for purchasing electric vehicles?\\\\\",\\\\\"tasks\\\\\":[\\\\\"SEARCH: Find a list of countries with incentives for purchasing electric vehicles\\\\\"],\\\\\"can_i_answer\\\\\":false}', expected_output='{\"question\": \"Which countries currently offer incentives for purchasing electric vehicles?\", \"tasks\": [\"RESEARCH: Identify a list of countries that offer electric vehicle incentives\", \"COMPARE: Examine the types of incentives each country offers\"], \"can_i_answer\": false}', context=None, retrieval_context=None)]\n"
     ]
    }
   ],
   "source": [
    "# input={\"question\":\"\\u00bfExisten incentivos para la instalaci\\u00f3n de estaciones de carga de veh\\u00edculos el\\u00e9ctricos en Italia en 2023?\"}\n",
    "\n",
    "# # input=json.dumps(input).strip()\n",
    "\n",
    "# prompt_template = f\"\"\"<<SYS>> Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false. <<SYS>> [INST] {input} [/INST] \"\"\"\n",
    "\n",
    "\n",
    "# output={\"question\":\"\\u00bfExisten incentivos para la instalaci\\u00f3n de estaciones de carga de veh\\u00edculos el\\u00e9ctricos en Italia en 2023?\",\"tasks\":[\"RESEARCH: Investigate whether there are incentives for installing electric vehicle charging stations in Italy in 2023\"],\"can_i_answer\":False}\n",
    "\n",
    "# actual_output=get_llm_response(prompt_template)\n",
    "\n",
    "import pandas as pd\n",
    "directory_path = Path(\"eval_dataset_for_evaluation_metric\")\n",
    "testing_file_path = directory_path / \"testing.json\"\n",
    "'''\n",
    "df=pd.read_json(testing_file_path)\n",
    "\n",
    "# print(df)\n",
    "df['Status'] = None\n",
    "df['Status'] = None\n",
    "'''\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_cases_from_json_file(\n",
    "    file_path=testing_file_path,\n",
    "    input_key_name=\"questions\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    # context_key_name=\"expected_output\",\n",
    "    # retrieval_context_key_name=\"expected_output\",\n",
    ")\n",
    "\n",
    "# results=evaluate(dataset, [answerrelevancy_metric,geval_metric,custom_metric])\n",
    "results=evaluate(dataset, [answerrelevancy_metric,geval_question_undertsanding_metric,geval_format_metric])\n",
    "print(results)\n",
    "# df = pd.DataFrame()\n",
    "# for i in results:\n",
    "#     print(i.input)\n",
    "#     print(i.actual_output)\n",
    "#     print(i.context)\n",
    "#     # print(i.metrics)\n",
    "#     print(i.success)\n",
    "#     for j in i.metrics:\n",
    "#         print(j.score)\n",
    "#         print(j.reason)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Evaluation Metric Results:\")\n",
    "# print(results)\n",
    "# for index, row in df.iterrows():\n",
    "#     test_case1 = LLMTestCase(\n",
    "#         input=row[\"questions\"],\n",
    "#         # Replace this with the actual output from your LLM application\n",
    "#         actual_output=row[\"actual_output\"],\n",
    "#         context=[json.dumps(row[\"expected_output\"])]\n",
    "#     )\n",
    "#     results=evaluate([test_case1], [answerrelevancy_metric,hallucination_metric,geval_metric])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940104f2-454d-4610-8085-867ebc8b0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# print(type(i.success))\n",
    "# data={\"input\":[result.input for result in result],\"actual_output\":[result.actual_output for result in result],\"context\":[result.context for result in result]}\n",
    "# data1={ x:y for x, y in zip((\"input\",\"actual_output\",\"context\"),[i.input,i.actual_output,i.context] for i in results)} | {\"overall_status\": [\"Passed\" for i in results if i.success else \"Failed\"]} | {x:y for x, y in zip((f\"{j.__name__} status\",f\"{j.__name__} score\",f\"{j.__name__} reason\"),(j.input,j.actual_output,j.context)) for i in results for j in i.metrics}\n",
    "\n",
    "new_dict = defaultdict(list)\n",
    "# print(data)\n",
    "\n",
    "\n",
    "for i in results:\n",
    "    new_dict[\"input\"].append(i.input)\n",
    "    new_dict[\"actual_output\"].append(i.actual_output)\n",
    "    new_dict[\"context\"].append(i.context)\n",
    "    if i.success:\n",
    "        new_dict[\"overall_status\"].append(\"Paased\")\n",
    "    else:\n",
    "        new_dict[\"overall_status\"].append(\"Failed\")\n",
    "    for j in i.metrics:\n",
    "        new_dict[f\"{j.__name__} status\"].append(j.is_successful())\n",
    "        new_dict[f\"{j.__name__} score\"].append(j.score)\n",
    "        new_dict[f\"{j.__name__} reason\"].append(j.reason)\n",
    "\n",
    "\n",
    "# print(new_dict)\n",
    "df = pd.DataFrame(new_dict)\n",
    "\n",
    "current_time = datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "csv_path=directory_path / f\"eval_test_run_{current_time}.csv\"\n",
    "df.to_csv(csv_path,sep=\"|\",index=False,header=True,mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55080d0-66c0-4ce3-b593-7b38cb535f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in results:\n",
    "#     print(i.input)\n",
    "#     print(i.actual_output)\n",
    "#     print(i.context)\n",
    "#     # print(i.metrics)\n",
    "#     print(i.success)\n",
    "#     for j in i.metrics:\n",
    "#         print(j.score)\n",
    "#         print(j.reason)\n",
    "# print(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3b59c-3619-41ec-b4b0-66d86d2edac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
