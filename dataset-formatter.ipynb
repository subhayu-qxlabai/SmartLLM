{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# dataset_1 = Dataset.from_json(\"datasets/llm1_dataset.jsonl\")\n",
    "dataset_2 = Dataset.from_json(\"datasets/llm2_dataset.jsonl\")\n",
    "# dataset_3 = Dataset.from_json(\"datasets/llm3_dataset.jsonl\")\n",
    "# all_datasets = {\"llm1\": dataset_1, \"llm2\": dataset_2, \"llm3\": dataset_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 25548\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': \"You, being an honest and intelligent assistant, are tasked with creating steps for accomplishment. There's no requirement for executing the functions; instead, organize them logically using your cognitive abilities. The output format should be strictly in JSON.\"},\n",
       "  {'from': 'human',\n",
       "   'value': '{\"query\": \"What are the latest developments in quantum computing as of 2023?\", \"steps\": [\"RESEARCH: Investigate the most recent advancements in quantum computing technology\"], \"functions\": [{\"name\": \"SCI_TECH_INNOVATIONS\", \"description\": \"Provides details on recent scientific and technological advancements in various fields.\", \"parameters\": {\"type\": \"object\", \"properties\": [{\"name\": \"field\", \"type\": \"string\", \"description\": \"The specific field of science or technology to get updates on, such as \\'Biotechnology\\', \\'Artificial Intelligence\\', or \\'Renewable Energy\\'.\"}, {\"name\": \"date_range\", \"type\": \"string\", \"description\": \"The range of dates to search for innovations (format: YYYY-MM-DD to YYYY-MM-DD).\"}, {\"name\": \"keywords\", \"type\": \"array\", \"description\": \"A list of keywords to refine the search for innovations.\"}], \"required\": [\"field\"]}}, {\"name\": \"TECHNOLOGY_INFORMATION_RETRIEVAL\", \"description\": \"Fetches detailed information about specific technologies, including specifications, applications, and advancements.\", \"parameters\": {\"type\": \"object\", \"properties\": [{\"name\": \"technology_name\", \"type\": \"string\", \"description\": \"The name of the technology to retrieve information for.\"}, {\"name\": \"aspect\", \"type\": \"string\", \"description\": \"The particular aspect of the technology to inquire about, such as \\'specifications\\', \\'history\\', or \\'current_trends\\'.\"}], \"required\": [\"technology_name\"]}}, {\"name\": \"TECHNOLOGY_REPORT_GENERATOR\", \"description\": \"Compiles technology reports based on recent developments, trends, and analyses.\", \"parameters\": {\"type\": \"object\", \"properties\": [{\"name\": \"topics\", \"type\": \"array\", \"description\": \"A list of technology topics to be included in the report.\"}, {\"name\": \"depth\", \"type\": \"string\", \"description\": \"The level of depth for the report, either a \\'summary\\' or a \\'detailed\\' analysis.\"}], \"required\": [\"topics\"]}}, {\"name\": \"FORMAT_TEXT_DATA\", \"description\": \"Formats and structures text data according to specified formatting rules.\", \"parameters\": {\"type\": \"object\", \"properties\": [{\"name\": \"text\", \"type\": \"string\", \"description\": \"The block of text to be formatted.\"}, {\"name\": \"format_style\", \"type\": \"string\", \"description\": \"The desired text formatting style, such as \\'plain\\', \\'markdown\\', or \\'html\\'.\"}], \"required\": [\"text\"]}}]}'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '{\"overview\": \"To uncover the latest developments in quantum computing as of 2023, we must research recent scientific and technological advancements, retrieve in-depth information specific to quantum computing, generate a comprehensive report on these developments, and finally format the data for presentation.\", \"steps\": [{\"id\": \"step_1\", \"thought\": \"We should start by identifying the most recent scientific and technological advancements in the field of quantum computing.\", \"explore_tools\": \"A tool that provides updates on the latest scientific and technological innovations can be used to acquire this information.\", \"available_tools\": [\"SCI_TECH_INNOVATIONS\"], \"choose_tool\": \"The \\'SCI_TECH_INNOVATIONS\\' tool will be used because it specializes in providing the latest updates in specific fields, including quantum computing.\", \"understand_dependencies\": null, \"extract\": [], \"function\": [{\"id\": \"function_1\", \"name\": \"SCI_TECH_INNOVATIONS\", \"parameters\": [{\"name\": \"field\", \"value\": \"Quantum Computing\"}, {\"name\": \"date_range\", \"value\": \"2023-01-01 to 2023-12-31\"}, {\"name\": \"keywords\", \"value\": [\"quantum supremacy\", \"quantum entanglement\", \"qubit\", \"quantum algorithm\"]}]}]}, {\"id\": \"step_2\", \"thought\": \"After identifying recent innovations, we should gather detailed information specific to the advancements in quantum computing.\", \"explore_tools\": \"A tool that retrieves detailed information about specific technologies can be used to delve deeper into quantum computing advancements.\", \"available_tools\": [\"TECHNOLOGY_INFORMATION_RETRIEVAL\"], \"choose_tool\": \"The \\'TECHNOLOGY_INFORMATION_RETRIEVAL\\' tool will be selected to get comprehensive details on the latest developments in quantum computing.\", \"understand_dependencies\": null, \"extract\": [], \"function\": [{\"id\": \"function_2\", \"name\": \"TECHNOLOGY_INFORMATION_RETRIEVAL\", \"parameters\": [{\"name\": \"technology_name\", \"value\": \"Quantum Computing\"}, {\"name\": \"aspect\", \"value\": \"current_trends\"}]}]}, {\"id\": \"step_3\", \"thought\": \"Now it\\'s time to compile a report based on the information gathered from the research and in-depth analysis of quantum computing advancements.\", \"explore_tools\": \"A report generator can be used to compile a structured report summarizing the recent developments in quantum computing.\", \"available_tools\": [\"TECHNOLOGY_REPORT_GENERATOR\"], \"choose_tool\": \"The \\'TECHNOLOGY_REPORT_GENERATOR\\' tool will be used to create a comprehensive report on the latest developments in quantum computing.\", \"understand_dependencies\": null, \"extract\": [], \"function\": [{\"id\": \"function_3\", \"name\": \"TECHNOLOGY_REPORT_GENERATOR\", \"parameters\": [{\"name\": \"topics\", \"value\": [\"Quantum Computing\"]}, {\"name\": \"depth\", \"value\": \"detailed\"}]}]}, {\"id\": \"step_4\", \"thought\": \"Finally, we should format the report to ensure it is clearly presented and easy to read.\", \"explore_tools\": \"A text formatting tool can be employed to structure and present the report according to standard formatting styles.\", \"available_tools\": [\"FORMAT_TEXT_DATA\"], \"choose_tool\": \"The \\'FORMAT_TEXT_DATA\\' tool will be used to format the report in a reader-friendly style, such as markdown or html.\", \"understand_dependencies\": null, \"extract\": [], \"function\": [{\"id\": \"function_4\", \"name\": \"FORMAT_TEXT_DATA\", \"parameters\": [{\"name\": \"text\", \"value\": \"{{step_3.function.function_3.output}}\"}, {\"name\": \"format_style\", \"value\": \"markdown\"}]}]}]}'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_json(\"datasets/llm2_sharegpt.jsonl\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.formatter import MessagesFormatter, DatasetFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MessagesFormatter(dataset_1[:1]['messages'], separator=\"\\n\").format().tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> Embodying the role of an earnest and brainy assistant who can reframe questions into actionable components, you face a question framed in JSON. You are to issue a response as a JSON object, consisting of can_i_answer (true illustrates the question is answerable independently, or false if it leans on external resources) and tasks, enumerating procedures to tackle the question using external sources, contingent on can_i_answer being false. <<SYS>>\n",
      "[INST] {\"question\": \"2023 Nobel Prize winners in Physics announced?\"} [/INST]\n",
      "{\"can_i_answer\": false, \"tasks\": [\"SEARCH: Check for the announcement of Nobel Prize winners in Physics for 2023\"], \"response\": null}\n"
     ]
    }
   ],
   "source": [
    "print(mf.formatted_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf1 = MessagesFormatter(\n",
    "    [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"systemmessage\"},\n",
    "            {\"role\": \"user\", \"content\": \"userinput\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"assistantoutput\"},\n",
    "        ]\n",
    "    ],\n",
    "    separator=\"\\n\",\n",
    "    system_template=\"<|im_start|>system\\n{system}<|im_end|>\",\n",
    "    user_template=\"<|im_start|>user\\n{user}<|im_end|>\",\n",
    "    assistant_template=\"<|im_start|>assistant\\n{assistant}<|im_end|>\",\n",
    ").format().tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "p = Path(r\"D:\\SmartLLM\\data\\functions\")\n",
    "funcs = list(chain(*[json.load(open(p)) for p in p.rglob(\"*.json\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format:   0%|          | 0/26 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 26/26 [00:04<00:00,  5.51ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136208104"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_alpaca_format(d: dict):\n",
    "    messages = d.pop(\"messages\", [])\n",
    "    return {\"system\": messages[0][\"content\"], \"input\": messages[1][\"content\"], \"output\": messages[2][\"content\"]}\n",
    "\n",
    "dataset_2.map(convert_to_alpaca_format).to_json(\"datasets/llm2_alpaca.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '<<SYS>> Embodying the role of an earnest and brainy assistant who can reframe questions into actionable components, you face a question framed in JSON. You are to issue a response as a JSON object, consisting of can_i_answer (true illustrates the question is answerable independently, or false if it leans on external resources) and tasks, enumerating procedures to tackle the question using external sources, contingent on can_i_answer being false. <<SYS>>\\n[INST] {\"question\": \"2023 Nobel Prize winners in Physics announced?\"} [/INST]\\n'\n",
    "output = '<s> <<SYS>> Embodying the role of an earnest and brainy assistant who can reframe questions into actionable components, you face a question framed in JSON. You are to issue a response as a JSON object, consisting of can_i_answer (true illustrates the question is answerable independently, or false if it leans on external resources) and tasks, enumerating procedures to tackle the question using external sources, contingent on can_i_answer being false. <<SYS>>\\n[INST] {\"question\": \"2023 Nobel Prize winners in Physics announced?\"} [/INST]\\n{\"can_i_answer\": false, \"tasks\": [\"SEARCH: Check for the announcement of Nobel Prize winners in Physics for 2023\"], \"response\": null} </s>'\n",
    "extract = '{\"question\": \"2023 Nobel Prize winners in Physics announced?\"} [/INST] {\"can_i_answer\": false, \"tasks\": [\"SEARCH: Check for the announcement of Nobel Prize winners in Physics for 2023\"], \"response\": null}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: <<SYS>> {system} <<SYS>>\n",
      "user: [INST] {human} [/INST]\n",
      "assistant: {gpt}\n"
     ]
    }
   ],
   "source": [
    "mf.formatted_messages[0]\n",
    "\n",
    "mf.base_format\n",
    "\n",
    "attr_names = [\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "for attr_name in attr_names:\n",
    "    print(f\"{attr_name}: {getattr(mf, f'{attr_name}_template')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[1;32m----> 4\u001b[0m parse(\u001b[43mmf\u001b[49m\u001b[38;5;241m.\u001b[39mbase_format, mf\u001b[38;5;241m.\u001b[39mbase_format)\u001b[38;5;241m.\u001b[39mnamed\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mf' is not defined"
     ]
    }
   ],
   "source": [
    "from parse import parse\n",
    "\n",
    "\n",
    "parse(mf.base_format, mf.base_format).named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse(\"{name} is {adjective}, I {} her!\", \"Hanna is lovely, I her!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '{system} <<SYS>>\\n[INST] {human} [/INST]\\n{gpt}'\n",
    "formatted_text = 'system <<SYS>>\\n[INST] human [/INST]\\ngpt'\n",
    "\n",
    "template = \"{name} is {adjective}, I {verb} her {items}!\"\n",
    "formatted_text = \"Hanna is lovely, I love her eyes!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'Embodying the role of an earnest and brainy assistant who can reframe questions into actionable components, you face a question framed in JSON. You are to issue a response as a JSON object, consisting of can_i_answer (true illustrates the question is answerable independently, or false if it leans on external resources) and tasks, enumerating procedures to tackle the question using external sources, contingent on can_i_answer being false.',\n",
       " 'user': '{\"question\": \"2023 Nobel Prize winners in Physics announced?\"}',\n",
       " 'assistant': '{\"can_i_answer\": false, \"tasks\": [\"SEARCH: Check for the announcement of Nobel Prize winners in Physics for 2023\"], \"response\": null}'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.text_utils import TextUtils\n",
    "\n",
    "TextUtils.parse_to_dict(mf.base_format, mf.formatted_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from datasets import Dataset\n",
    "from models.inputs import StepsInput\n",
    "from models.outputs import StepsOutput\n",
    "\n",
    "dataset_2 = Dataset.from_json(\"datasets/llm2_dataset.jsonl\")\n",
    "d = {\n",
    "    StepsInput.model_validate_json(\n",
    "        x[\"messages\"][1][\"content\"]\n",
    "    ): StepsOutput.model_validate_json(x[\"messages\"][2][\"content\"])\n",
    "    for x in dataset_2\n",
    "}\n",
    "\n",
    "inconsistent = {\n",
    "    f\"{hash(i)}{hash(o)}\": (\n",
    "        i.query,\n",
    "        set(chain(*[x.available_tools for x in o.steps]))\n",
    "        - set([f.name for f in i.functions]),\n",
    "    )\n",
    "    for ix, (i, o) in enumerate(d.items())\n",
    "    if set(chain(*[x.available_tools for x in o.steps]))\n",
    "    - set([f.name for f in i.functions])\n",
    "    != set()\n",
    "}\n",
    "indexes_to_remove = list(inconsistent)\n",
    "df_2 = dataset_2.to_pandas()\n",
    "# del dataset_2\n",
    "# df_2_dict = df_2.drop(indexes_to_remove).to_dict(orient=\"records\")\n",
    "# dataset_2 = Dataset.from_list(\n",
    "#     [{\"messages\": list(x.get(\"messages\", []))} for x in df_2_dict]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['hash'] = df_2['messages'].apply(lambda x: f\"{hash(StepsInput.model_validate_json(x[1]['content']))}{hash(StepsOutput.model_validate_json(x[2]['content']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = Dataset.from_pandas(df_2.set_index('hash', drop=True).drop(indexes_to_remove).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format:   0%|          | 0/26 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 26/26 [00:02<00:00,  9.34ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135693628"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2.to_json(\"datasets/llm2_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25122/25122 [00:06<00:00, 3769.38 examples/s]\n",
      "Creating json from Arrow format: 100%|██████████| 26/26 [00:02<00:00, 11.22ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "133884844"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2.map(convert_to_alpaca_format).to_json(\"datasets/llm2_alpaca.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SmartLLM\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\SmartLLM\\env\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:184: UserWarning: Field name \"schema\" shadows an attribute in parent \"BaseModel\"; \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicGenerator num tokens: 170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Renewable Energy Sources',\n",
       " 'Virtual Reality Gaming',\n",
       " 'Cancer Research Breakthroughs',\n",
       " 'Cryptocurrency Trends',\n",
       " 'Mindfulness Meditation Benefits',\n",
       " 'Global Warming Impact',\n",
       " 'Artificial Intelligence Ethics',\n",
       " 'Fashion Trends 2022',\n",
       " 'Healthy Meal Prep Ideas',\n",
       " 'Remote Work Productivity Tips']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset_gen import TopicGenerator\n",
    "from call_openai_lc import call_openai_api\n",
    "\n",
    "tg = TopicGenerator(openai_func=call_openai_api)\n",
    "tg.generate(10, dump=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionGenerator num tokens: 240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What are the current rent versus buy housing market trends in the San Francisco Bay Area for 2023?',\n",
       " 'Is it more cost-effective to rent or buy a home in the San Francisco Bay Area as of the latest data?',\n",
       " 'What are the projected long-term financial benefits of renting versus buying in the San Francisco Bay Area?']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset_gen import DatasetGenerator, QuestionGenerator\n",
    "\n",
    "dg = DatasetGenerator()\n",
    "dg.generate(\"In the san francisco bay area, does it make sense to rent or buy ?\", n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.txt', 'a+') as file:\n",
    "    file.write('This is a new line\\n')\n",
    "    file.write('This is another line\\n')\n",
    "\n",
    "    # Move the cursor to the beginning of the file\n",
    "    file.seek(0)\n",
    "\n",
    "    # Read the contents of the file\n",
    "    contents = file.read()\n",
    "    print(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llm_dataset import LLMDataset, LLMType, Dataset\n",
    "\n",
    "llm1_data = Dataset.from_json(\"/home/subhayu/Downloads/SmartLLM/dataset/llm1_alpaca.jsonl\")\n",
    "llm1_data = LLMDataset.from_dataset(llm1_data, LLMType.LLM1)\n",
    "llm2_data = Dataset.from_json(\"/home/subhayu/Downloads/SmartLLM/dataset/llm2_alpaca.jsonl\")\n",
    "llm2_data = LLMDataset.from_dataset(llm2_data, LLMType.LLM2)\n",
    "llm3_data = Dataset.from_json(\"/home/subhayu/Downloads/SmartLLM/dataset/llm3_alpaca.jsonl\")\n",
    "llm3_data = LLMDataset.from_dataset(llm3_data, LLMType.LLM3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1_datat = llm1_data.get_llm_type_rows(LLMType.LLM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMDataset(rows=46469)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLM1DatasetRow(llm=llm1, system=Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false., input=question='What is the GDP growth rate of India in 2021?', output=question='What is the GDP growth rate of India in 2021?' tasks=['SEARCH,DATA: Retrieve the GDP growth rate data for India for the year 2021'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=In your capacity as a sincere and intelligent assistant expert in breaking down complex queries into accessible steps, your task involves receiving a question in JSON format. You're required to generate a JSON object as a response, featuring two key elements: can_i_answer (true if the question can be addressed independently of external resources or false if external resources are necessary) and tasks, which enumerates the steps to answer the question using outside sources if can_i_answer concludes with false., input=question='What was the impact of the Chernobyl nuclear disaster?', output=question='What was the impact of the Chernobyl nuclear disaster?' tasks=['RESEARCH,HISTORY: Study the historical impact of the Chernobyl disaster'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=Your role as an upright and sharp-witted assistant who can distill questions into simple procedural steps involves handling an input question in JSON. You must deliver a JSON object in response, equipped with the keys can_i_answer (indicating true if the question is answerable with the resources at hand, or false if not) and tasks, outlining the process for answering the question with external resources if can_i_answer is false., input=question='What is the current exchange rate between Euro and USD?', output=question='What is the current exchange rate between Euro and USD?' tasks=['SEARCH,FINANCE: Look up the latest exchange rate between EUR and USD'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=Embodying the role of an earnest and brainy assistant who can reframe questions into actionable components, you face a question framed in JSON. You are to issue a response as a JSON object, consisting of can_i_answer (true illustrates the question is answerable independently, or false if it leans on external resources) and tasks, enumerating procedures to tackle the question using external sources, contingent on can_i_answer being false., input=question='What was the most recent major pandemic outbreak?', output=question='What was the most recent major pandemic outbreak?' tasks=[] can_i_answer=True),\n",
       " LLM1DatasetRow(llm=llm1, system=Taking on the responsibility as an honest and astute assistant skilled in simplifying queries into digestible steps, you are presented with a JSON input question. You are obligated to supply a JSON object as a return, carrying the keys can_i_answer (true insinuates that the question can be answered with no need for external resources, or false if otherwise) and tasks, listing necessary steps for answering the question with external help if can_i_answer is established as false., input=question='What is the average salary in the technology industry in the United Kingdom?', output=question='What is the average salary in the technology industry in the United Kingdom?' tasks=['SEARCH,DATA: Find current average salary data for the technology sector in the UK'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=You are an honest and smart assistant who can break down questions into simple steps. You are given a question in JSON in input and you have to return a JSON object as a response containing the keys can_i_answer (should be true if you can answer without external resources or false if the question cannot be answered without external resources) and tasks (should be a list of steps to answer the input question using external sources if can_i_answer is false), input=question='Who is the CEO of Amazon?', output=question='Who is the CEO of Amazon?' tasks=[] can_i_answer=True),\n",
       " LLM1DatasetRow(llm=llm1, system=Taking on the responsibility as an honest and astute assistant skilled in simplifying queries into digestible steps, you are presented with a JSON input question. You are obligated to supply a JSON object as a return, carrying the keys can_i_answer (true insinuates that the question can be answered with no need for external resources, or false if otherwise) and tasks, listing necessary steps for answering the question with external help if can_i_answer is established as false., input=question='What is the current average life expectancy in Canada?', output=question='What is the current average life expectancy in Canada?' tasks=['SEARCH,DATA: Locate the most recent data on life expectancy in Canada'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=Being an honest and smart assistant talented in breaking down questions into actionable items, you're charged with interpreting a JSON-formatted question. Your output must be a JSON object articulated with two keys: can_i_answer (indicating true if the inquiry is answerable using internal capabilities, or false if it requires external resources) and tasks, delineating the series of steps to answer the question with external aids if can_i_answer is false., input=question='What is the exchange rate of EUR to JPY?', output=question='What is the exchange rate of EUR to JPY?' tasks=['SEARCH,FINANCE: Check the current exchange rate from Euro to Japanese Yen'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=Your role as an upright and sharp-witted assistant who can distill questions into simple procedural steps involves handling an input question in JSON. You must deliver a JSON object in response, equipped with the keys can_i_answer (indicating true if the question is answerable with the resources at hand, or false if not) and tasks, outlining the process for answering the question with external resources if can_i_answer is false., input=question='What was the medal tally of the 2000 Sydney Olympics?', output=question='What was the medal tally of the 2000 Sydney Olympics?' tasks=['SEARCH,HISTORY: Find the official medal tally of the 2000 Sydney Olympics'] can_i_answer=False),\n",
       " LLM1DatasetRow(llm=llm1, system=Operating as an honest and clever assistant with the ability to deconstruct questions into straightforward steps, you are tasked to process a question presented in JSON format. Your response should be structured as a JSON object containing two keys: can_i_answer (true if the question can be addressed without external resources, or false otherwise), and tasks, a sequence of steps that detail how to answer the query using external sources, should can_i_answer be set to false., input=question='What important development took place in the year 1984 in India?', output=question='What important development took place in the year 1984 in India?' tasks=['RESEARCH,HISTORY: Investigate significant events in India during 1984'] can_i_answer=False)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm1_datat[0:10].rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
